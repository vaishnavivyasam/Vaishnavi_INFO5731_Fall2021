{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In-class-exercise-03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaishnavivyasam/Vaishnavi_INFO5731_Fall2021/blob/main/In_class_exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A7y3Yt7Pl-F"
      },
      "source": [
        "## The third In-class-exercise (9/29/2021, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khuBO2mUPl-K"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhMlhiLHPl-N"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZsbXPKpPl-O"
      },
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Text classification is one of the fundamental tasks in natural language processing that is used to organize, structure, and \n",
        "categorize any kind of text data from documents, files etc. The text classification task I used here is the 'Sentiment analysis'. \n",
        "Sentiment analysis is the process of detecting positive or negative sentiment in text. It’s often used by businesses to detect sentiment in \n",
        "social data, gauge brand reputation, and understand customers. There are different types of sentiment analysis, namely\n",
        "1.Fine-grained Sentiment Analysis\n",
        "2.Emotion detection\n",
        "3.Aspect-based Sentiment Analysis\n",
        "4.Multilingual sentiment analysis\n",
        "Sentiment analysis is extremely important because it allows businesses to understand the sentiment of their customers \n",
        "towards their brand. By automatically sorting the sentiment behind social media conversations, reviews, and more, businesses can make \n",
        "better and more informed decisions.\n",
        "For the sentiment analysis, I am planning to use 5 different features like \n",
        "1.entity feature \n",
        "2.dependency parshing feature\n",
        "3.POS tagging feature\n",
        "4.Bag of words feature\n",
        "5.tf-idf feature.\n",
        "  \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgv2ZA8cPl-S"
      },
      "source": [
        "Question 2 (20 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIItz7_iA3Vl",
        "outputId": "7a5909ed-0cd0-4ba5-ea81-309669988f2b"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XjlIPszE0EQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a63c7dc-bcf7-4f63-b885-e0fc4d101450"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R72DtPwsBic2",
        "outputId": "4bfb69f6-1c41-4dc2-9577-e7233f534c82"
      },
      "source": [
        "#1.Entity feature extarction\n",
        "\n",
        "import spacy\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "entity_types = ['PERSON','NORP','FAC','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW',\n",
        "                'LANGUAGE','DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']\n",
        "\n",
        "entity_name = ['person','Nationality','Building','Institution','country','location','PRODUCT','EVENT','Title','LAW',\n",
        "                'LANGUAGE','DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']\n",
        "\n",
        "#defining a function to classify the key elemnts\n",
        "def entity_extraction(sentence):\n",
        "    entity_list = []\n",
        "    doc = nlp(sentence)\n",
        "#returning all the entities\n",
        "    for ent in doc.ents:\n",
        "        entity_list.append([ent.text, ent.start_char, ent.end_char, ent.label_])\n",
        "    \n",
        "    return entity_list\n",
        "\n",
        "sentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "\n",
        "entity_list = entity_extraction(sentence)\n",
        "print(entity_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Apple', 0, 5, 'ORG'], ['U.K.', 27, 31, 'GPE'], ['$1 billion', 44, 54, 'MONEY']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfDZFzDyBsLh",
        "outputId": "644323a5-e453-4aaa-de17-bf7bd407fdc8"
      },
      "source": [
        "#2.dependancy parsing feature extraction\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "            [child for child in token.children])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autonomous amod cars NOUN []\n",
            "cars nsubj shift VERB [Autonomous]\n",
            "shift ROOT shift VERB [cars, liability]\n",
            "insurance compound liability NOUN []\n",
            "liability dobj shift VERB [insurance, toward]\n",
            "toward prep liability NOUN [manufacturers]\n",
            "manufacturers pobj toward ADP []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFKHYFe4BvUz",
        "outputId": "6d1f3851-4525-4ee8-ff3d-3cf02d8bd579"
      },
      "source": [
        "import spacy\n",
        "from spacy.symbols import nsubj, VERB\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "\n",
        "# Finding a verb with a subject \n",
        "verbs = set()\n",
        "for possible_subject in doc:\n",
        "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
        "        verbs.add(possible_subject.head)\n",
        "print(verbs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{shift}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_RmMRFxByyE",
        "outputId": "84645389-158b-4a7f-e853-7ea00b291bb6"
      },
      "source": [
        "#3.parts of speech tagging feature extraction\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "             token.is_alpha, token.is_stop)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple Apple PROPN NNP nsubj True False\n",
            "is be AUX VBZ aux True True\n",
            "looking look VERB VBG ROOT True False\n",
            "at at ADP IN prep True True\n",
            "buying buy VERB VBG pcomp True False\n",
            "U.K. U.K. PROPN NNP compound False False\n",
            "startup startup NOUN NN dobj True False\n",
            "for for ADP IN prep True True\n",
            "$ $ SYM $ quantmod False False\n",
            "1 1 NUM CD compound False False\n",
            "billion billion NUM CD pobj True False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07UigvYwB3Oe",
        "outputId": "96f6dba1-d8ee-4da4-ffb8-53d8ce21a54b"
      },
      "source": [
        "#4.bag of words feature extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#assigning the attribute values \n",
        "bow = CountVectorizer(max_features=50, lowercase=True, ngram_range=(1,3),analyzer = \"word\") \n",
        "sentence = [\"We are good','We are becoming better','We will be great\" ]\n",
        "train_bow = bow.fit_transform(sentence)\n",
        "# getting feature names from bag of words of the document\n",
        "bow.get_feature_names() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['are',\n",
              " 'are becoming',\n",
              " 'are becoming better',\n",
              " 'are good',\n",
              " 'are good we',\n",
              " 'be',\n",
              " 'be great',\n",
              " 'becoming',\n",
              " 'becoming better',\n",
              " 'becoming better we',\n",
              " 'better',\n",
              " 'better we',\n",
              " 'better we will',\n",
              " 'good',\n",
              " 'good we',\n",
              " 'good we are',\n",
              " 'great',\n",
              " 'we',\n",
              " 'we are',\n",
              " 'we are becoming',\n",
              " 'we are good',\n",
              " 'we will',\n",
              " 'we will be',\n",
              " 'will',\n",
              " 'will be',\n",
              " 'will be great']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "QSvuj_2UCFP5",
        "outputId": "b78950be-8d94-4967-df97-a83f6a87ed97"
      },
      "source": [
        "#5.tf-idf feature extraction\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "#assigning the countvectorizer class functionality\n",
        "vec = CountVectorizer(binary=True) \n",
        "texts = [\"We are good','We are becoming better','We will be great\"]\n",
        "\n",
        "#fitting the data with the function\n",
        "vec.fit(texts)\n",
        "print([w for w in sorted(vec.vocabulary_.keys())])\n",
        "\n",
        "pd.DataFrame(vec.transform(texts).toarray(), columns=sorted(vec.vocabulary_.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['are', 'be', 'becoming', 'better', 'good', 'great', 'we', 'will']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>are</th>\n",
              "      <th>be</th>\n",
              "      <th>becoming</th>\n",
              "      <th>better</th>\n",
              "      <th>good</th>\n",
              "      <th>great</th>\n",
              "      <th>we</th>\n",
              "      <th>will</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   are  be  becoming  better  good  great  we  will\n",
              "0    1   1         1       1     1      1   1     1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "MQw2rFr6CTEx",
        "outputId": "09255d0c-acdf-4917-88ce-f3925968bb94"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#calculating tf-idf values\n",
        "vec = TfidfVectorizer()\n",
        "#assigning the data\n",
        "vec.fit(texts)\n",
        "pd.DataFrame(vec.transform(texts).toarray(), columns=sorted(vec.vocabulary_.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>are</th>\n",
              "      <th>be</th>\n",
              "      <th>becoming</th>\n",
              "      <th>better</th>\n",
              "      <th>good</th>\n",
              "      <th>great</th>\n",
              "      <th>we</th>\n",
              "      <th>will</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.458831</td>\n",
              "      <td>0.229416</td>\n",
              "      <td>0.229416</td>\n",
              "      <td>0.229416</td>\n",
              "      <td>0.229416</td>\n",
              "      <td>0.229416</td>\n",
              "      <td>0.688247</td>\n",
              "      <td>0.229416</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        are        be  becoming  ...     great        we      will\n",
              "0  0.458831  0.229416  0.229416  ...  0.229416  0.688247  0.229416\n",
              "\n",
              "[1 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q6Iko8DPl-U"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\" Select the most important features you extracted above, rank the features based on their importance in the descending order. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBBL2YpPl-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef214713-4918-41cd-9ece-eaedb5499c3c"
      },
      "source": [
        "# You code here (Please add comments in the code\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "  \n",
        "# Loading iris data\n",
        "iris_dataset = load_iris()\n",
        "  \n",
        "# Creating features and target\n",
        "X = iris_dataset.data\n",
        "y = iris_dataset.target\n",
        "  \n",
        "# Converting to categorical data by converting data to integers\n",
        "X = X.astype(int)\n",
        "  \n",
        "# Two features with highest chi-squared statistics are selected\n",
        "chi2_features = SelectKBest(chi2, k = 2)\n",
        "X_kbest_features = chi2_features.fit_transform(X, y)\n",
        "  \n",
        "# Reduced features\n",
        "print('Original feature number:', X.shape[1])\n",
        "print('Reduced feature number:', X_kbest_features.shape[1])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original feature number: 4\n",
            "Reduced feature number: 2\n"
          ]
        }
      ]
    }
  ]
}