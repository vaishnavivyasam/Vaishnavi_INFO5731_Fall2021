{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In-class-exercise-02.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaishnavivyasam/Vaishnavi_INFO5731_Fall2021/blob/main/In_class_exercise_02(including_4thquestion%20).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg05xw3wLuA1"
      },
      "source": [
        "## The third In-class-exercise (9/15/2021, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5GvdeMjLuA4"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUYpfkk4LuA5"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2ZJ2mCkLuA5"
      },
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "I used the Amazon product reviews for machine learning book for question 2 and I used articles from CiteSeerX based  the topic'Web'.\n",
        "\n",
        "1.I downloaed the url, using beautiful soup to parse the data\n",
        "2.Then using inspect option, I located the block of review on the webpage and found \n",
        "the div class for each of the necessary items that are needed to be printed.\n",
        "3. I added the loop and mentioned the number of iterations to be done.\n",
        "4. I replaced all the un-necessary characters from the text to make it clean\n",
        "5. I also used strip to remove unwanted spaces from the text.\n",
        "6. I added all the output to a dataframe.\n",
        "\n",
        "P.S: I wanted to collect the 1000 reviews for a movie in IMDB, but I only got the 23 reviews as IMDB has 'load more' option instead of pages which became quite complicated, so\n",
        "I chose Amazon reviews which has 423 reviews for the book I selected.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZkfrbRgLuA6"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 reviews of a movie from IMDB (https://www.imdb.com/) or 1000 reviews of a product from Amazon (https://www.amazon.com/).\n",
        "\n",
        "As for the IMDB movie review, the following informtion need to be collected (for example: https://www.imdb.com/title/tt6751668/reviews?ref_=tt_urv):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time\n",
        "\n",
        "\n",
        "As for the Amazon product review, the following information need to be collected (for example: https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_3?crid=2E3C55VKJX0K3&dchild=1&keywords=machine+learning+andrew+ng&qid=1631718619&sr=8-3):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wHOJ5yjLuA7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "6f907cd1-4ca7-4ab0-cd2d-8dc8e08fc1a5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.ticker as ticker\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "no_pages = 43\n",
        "def get_data(pageno):  \n",
        "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
        "  \n",
        "    r = requests.get('https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/product-reviews/1492032646/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber='+str(pageno), headers=headers)#, proxies=proxies)\n",
        "    content = r.content\n",
        "    soup = BeautifulSoup(content)\n",
        "    #print(soup)\n",
        "    alls = []\n",
        "\n",
        "    for d in soup.findAll('div', attrs={'class':'a-section celwidget'}): # review window\n",
        "      \n",
        "      username = d.find('', attrs={'class':'a-profile-name'})\n",
        "      #print(username)\n",
        "      rating = d.find('span', attrs={'class':'a-icon-alt'})\n",
        "      title_parent = d.find('a', attrs={'class':'a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold'})\n",
        "      review_text_parent = d.find('span', attrs={'a-size-base review-text review-text-content'})\n",
        "      review_time = d.find('span', attrs={'a-size-base a-color-secondary review-date'})\n",
        "\n",
        "      all1=[]\n",
        "\n",
        "      if username is not None:\n",
        "            #print(rating.text)\n",
        "            all1.append(username.text)\n",
        "      else:\n",
        "            all1.append('Amazon Customer') # returns 'amazon customer' if there is no name to the user\n",
        "\n",
        "      if rating is not None:\n",
        "            #print(price.text)\n",
        "            all1.append(rating.text)\n",
        "      else:\n",
        "            all1.append('NA')     \n",
        "\n",
        "      if title_parent is not None:\n",
        "            title = title_parent.find('span')\n",
        "            if title_parent is not None:\n",
        "                #print(price.text)\n",
        "                all1.append(title.text.replace(\"\\\"\",\"\")) # replaces \\ \n",
        "            else:\n",
        "                all1.append('NA')\n",
        "      else:\n",
        "            all1.append('NA')\n",
        "      \n",
        "      if review_text_parent is not None:\n",
        "            #print(price.text)\n",
        "            review_text = review_text_parent.find('span')\n",
        "            if review_text is not None:\n",
        "                #print(price.text)\n",
        "                rev_text = review_text.text.replace(\"\\n\",\"\") # to replace \\n with empty space\n",
        "                all1.append(rev_text)\n",
        "            else:\n",
        "                all1.append('Not Avaialable')\n",
        "      else:\n",
        "            all1.append('Not Available')\n",
        "      \n",
        "      if review_time is not None:\n",
        "            #print(price.text)\n",
        "            #Replacing all the text data to print only date\n",
        "            posted_time = review_time.text.strip().replace(\"Reviewed in Japan on \",\"\").replace(\"Reviewed in the United States on\",\"\").replace(\"Reviewed in France on \",\"\")\n",
        "            all1.append(posted_time)\n",
        "      else:\n",
        "            all1.append('Not Available')\n",
        "\n",
        "      \n",
        "      alls.append(all1)    \n",
        "    return alls\n",
        "\n",
        "\n",
        "results = []\n",
        "for i in range(1, no_pages+1):\n",
        "  results.append(get_data(i))\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "df = pd.DataFrame(flatten(results),columns=['User name','Star','Review title','Review text', 'Review posted time'])\n",
        "df.shape\n",
        "df\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User name</th>\n",
              "      <th>Star</th>\n",
              "      <th>Review title</th>\n",
              "      <th>Review text</th>\n",
              "      <th>Review posted time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CDH</td>\n",
              "      <td>5.0 out of 5 stars</td>\n",
              "      <td>Best in class book</td>\n",
              "      <td>I've read all of the predominant machine lea...</td>\n",
              "      <td>October 2, 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Christopher G. Loverich</td>\n",
              "      <td>5.0 out of 5 stars</td>\n",
              "      <td>Absurdly good</td>\n",
              "      <td>This book gives you a hands-on approach to l...</td>\n",
              "      <td>December 31, 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Alex Dadgar</td>\n",
              "      <td>5.0 out of 5 stars</td>\n",
              "      <td>great book and nice ebook experience</td>\n",
              "      <td>This is an update for my previous review.Rec...</td>\n",
              "      <td>January 9, 2020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>shirzad malekpour</td>\n",
              "      <td>3.0 out of 5 stars</td>\n",
              "      <td>Math fonts in kindle edition</td>\n",
              "      <td>While I enjoy learning from this book, the m...</td>\n",
              "      <td>September 27, 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Jarrod</td>\n",
              "      <td>5.0 out of 5 stars</td>\n",
              "      <td>Best Machine Learning book I own</td>\n",
              "      <td>I'm very pleased with this book. I enjoy the...</td>\n",
              "      <td>November 17, 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>Matthieu</td>\n",
              "      <td>5.0 out of 5 stars</td>\n",
              "      <td>NA</td>\n",
              "      <td>This Book is a gold mine for engineers and s...</td>\n",
              "      <td>December 23, 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>Anderson Chaves</td>\n",
              "      <td>5.0 out of 5 stars</td>\n",
              "      <td>NA</td>\n",
              "      <td>Excellent material! The right balance betwee...</td>\n",
              "      <td>November 29, 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>Francesco</td>\n",
              "      <td>5.0 out of 5 stars</td>\n",
              "      <td>NA</td>\n",
              "      <td>This is a great book!</td>\n",
              "      <td>November 19, 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>Amazon カスタマー</td>\n",
              "      <td>5.0 out of 5 stars</td>\n",
              "      <td>NA</td>\n",
              "      <td>Masterpiece. Just perfect.</td>\n",
              "      <td>June 13, 2021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>Ibadurrahman</td>\n",
              "      <td>5.0 out of 5 stars</td>\n",
              "      <td>NA</td>\n",
              "      <td>This second edition book is totally worth yo...</td>\n",
              "      <td>December 15, 2020</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>430 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   User name  ...   Review posted time\n",
              "0                        CDH  ...      October 2, 2019\n",
              "1    Christopher G. Loverich  ...    December 31, 2019\n",
              "2                Alex Dadgar  ...      January 9, 2020\n",
              "3          shirzad malekpour  ...   September 27, 2019\n",
              "4                     Jarrod  ...    November 17, 2019\n",
              "..                       ...  ...                  ...\n",
              "425                 Matthieu  ...    December 23, 2019\n",
              "426          Anderson Chaves  ...    November 29, 2019\n",
              "427                Francesco  ...    November 19, 2019\n",
              "428             Amazon カスタマー  ...        June 13, 2021\n",
              "429             Ibadurrahman  ...    December 15, 2020\n",
              "\n",
              "[430 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCX2y6aALuA7"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/). \n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHOYDPtcLuA7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "a7164150-9f24-455d-aa4f-bb180bae81da"
      },
      "source": [
        "# Articles from CiteSeerX based on 'Web'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.ticker as ticker\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "iterations = 100 #no.of iterations\n",
        "\n",
        "def get_data(itr):  \n",
        "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
        "\n",
        "    r = requests.get('https://citeseerx.ist.psu.edu/search?q=web&t=doc&sort=rlv&start='+str(itr), headers=headers)#, proxies=proxies)\n",
        "    content = r.content\n",
        "    soup = BeautifulSoup(content)\n",
        "    #print(soup)\n",
        "\n",
        "    alls = []\n",
        "    for d in soup.findAll('div', attrs={'class':'result'}): # artcile window\n",
        "        all1=[]\n",
        "        #print(d)\n",
        "        title = d.find('a') # to get title\n",
        "        if title is not None:\n",
        "            tt = title.text.strip() # to remove the uncessary spaces from text\n",
        "            all1.append(tt)\n",
        "        else:\n",
        "            all1.append('Unknown') # returns unknows if there is no title\n",
        "      \n",
        "        venue = d.find('span', attrs={'class':'pubvenue'}) # to get venue of the book/article\n",
        "        if venue is not None:\n",
        "            pv = venue.text.strip()\n",
        "            pv= pv.replace(\"-\",\"\") # to replace unwanted '-' with empty space\n",
        "            all1.append(pv)\n",
        "        else:\n",
        "            all1.append('NA') # returns NA if there is no venue\n",
        "        #print(venue)\n",
        "\n",
        "        year = d.find('span', attrs={'class':'pubyear'}) # to get published year\n",
        "        if year is not None:\n",
        "            py = year.text.strip()\n",
        "            py= py.replace(\", \",\"\")\n",
        "            #print (py)\n",
        "            all1.append(py)\n",
        "        else:\n",
        "            all1.append('0000') # returns 0000 if there is no year\n",
        "        #print(year)\n",
        "\n",
        "        author = d.find('span', attrs={'class':'authors'}) # to get the authors of book/article\n",
        "        if author is not None:\n",
        "           \n",
        "            pa = author.text.strip()\n",
        "            pa= pa.replace(\"by \\n \",\"\")\n",
        "            all1.append(pa)\n",
        "        else:\n",
        "            all1.append('None')\n",
        "\n",
        "        abstract = d.find('div', attrs={'class':'snippet'}) # to get abstract\n",
        "        if abstract is not None:\n",
        "            ab = abstract.text.strip()\n",
        "            all1.append(ab)\n",
        "        else:\n",
        "            all1.append('Not Available')\n",
        "\n",
        "\n",
        "       \n",
        "        \n",
        "        alls.append(all1)\n",
        "\n",
        "  \n",
        "    return alls\n",
        "\n",
        "results = []\n",
        "for i in range(1, iterations+1):\n",
        "    results.append(get_data(i+10))\n",
        "# as the output will be a list, in order to pass it to the DataFrame it needs to be flattened\n",
        "flatten = lambda a: [item for sublist in a for item in sublist]\n",
        "df = pd.DataFrame(flatten(results),columns=['Title','Venue','Year','Author','Abstract'])\n",
        "df.shape\n",
        "df\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Venue</th>\n",
              "      <th>Year</th>\n",
              "      <th>Author</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rank Aggregation Methods for the Web</td>\n",
              "      <td>NA</td>\n",
              "      <td>2010</td>\n",
              "      <td>Cynthia Dwork,  Ravi Kumar, M...</td>\n",
              "      <td>\"... We consider the problem of combining rank...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Preparation for Mining World Wide Web Bro...</td>\n",
              "      <td>KNOWLEDGE AND INFORMATION SYSTEMS</td>\n",
              "      <td>1999</td>\n",
              "      <td>Robert Cooley, Bamshad Mobash...</td>\n",
              "      <td>\"... The World Wide Web (WWW) continues to gro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From SHIQ and RDF to OWL: The Making of a Web ...</td>\n",
              "      <td>Journal of Web Semantics</td>\n",
              "      <td>2003</td>\n",
              "      <td>Ian Horrocks,  Peter F. Patel...</td>\n",
              "      <td>\"... The OWL Web Ontology Language is a new fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mfold web server for nucleic acid folding and ...</td>\n",
              "      <td>Nucleic Acids Res</td>\n",
              "      <td>2003</td>\n",
              "      <td>Michael Zuker</td>\n",
              "      <td>\"... The abbreviated name,‘mfold web server’,d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Generating Representative Web Workloads for Ne...</td>\n",
              "      <td>NA</td>\n",
              "      <td>1997</td>\n",
              "      <td>Paul Barford, Mark Crovella</td>\n",
              "      <td>\"... One role for workload generation is as a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>Syskill &amp; Webert: Identifying interesting web ...</td>\n",
              "      <td>In Proc. 13th Natl. Conf. on Artificial Intel...</td>\n",
              "      <td>1998</td>\n",
              "      <td>Michael Pazzani, Jack Muramat...</td>\n",
              "      <td>\"... We describe Syskill &amp; Webert, a software ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Searching The Web: The Public and Their Queries</td>\n",
              "      <td>NA</td>\n",
              "      <td>2001</td>\n",
              "      <td>Amanda Spink,  Deitmar Wolfra...</td>\n",
              "      <td>\"... In studying actual Web searching by the p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Survey of clustering data mining techniques</td>\n",
              "      <td>NA</td>\n",
              "      <td>2002</td>\n",
              "      <td>Pavel Berkhin</td>\n",
              "      <td>\"... Accrue Software, Inc. Clustering is a div...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>httperf - A Tool for Measuring Web Server Perf...</td>\n",
              "      <td>In First Workshop on Internet Server Performance</td>\n",
              "      <td>1998</td>\n",
              "      <td>David Mosberger, Tai Jin</td>\n",
              "      <td>\"... This paper describes httperf, a tool for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>A database and evaluation methodology for opti...</td>\n",
              "      <td>In Proceedings of the IEEE International Conf...</td>\n",
              "      <td>2007</td>\n",
              "      <td>Simon Baker, Daniel Scharstei...</td>\n",
              "      <td>\"... The quantitative evaluation of optical fl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Title  ...                                           Abstract\n",
              "0                 Rank Aggregation Methods for the Web  ...  \"... We consider the problem of combining rank...\n",
              "1    Data Preparation for Mining World Wide Web Bro...  ...  \"... The World Wide Web (WWW) continues to gro...\n",
              "2    From SHIQ and RDF to OWL: The Making of a Web ...  ...  \"... The OWL Web Ontology Language is a new fo...\n",
              "3    Mfold web server for nucleic acid folding and ...  ...  \"... The abbreviated name,‘mfold web server’,d...\n",
              "4    Generating Representative Web Workloads for Ne...  ...  \"... One role for workload generation is as a ...\n",
              "..                                                 ...  ...                                                ...\n",
              "995  Syskill & Webert: Identifying interesting web ...  ...  \"... We describe Syskill & Webert, a software ...\n",
              "996    Searching The Web: The Public and Their Queries  ...  \"... In studying actual Web searching by the p...\n",
              "997        Survey of clustering data mining techniques  ...  \"... Accrue Software, Inc. Clustering is a div...\n",
              "998  httperf - A Tool for Measuring Web Server Perf...  ...  \"... This paper describes httperf, a tool for ...\n",
              "999  A database and evaluation methodology for opti...  ...  \"... The quantitative evaluation of optical fl...\n",
              "\n",
              "[1000 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNeukw1_LuA8"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjauhXvGLuA8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "2b7707d8-89ad-4a74-e46f-729059e6b036"
      },
      "source": [
        "# You code here (Please add comments in the code):\n",
        "#Importing required Libraries\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "text_query = 'Tesla'#search using keyword\n",
        "max_tweets = 1000 #number of tweets to be printed\n",
        "\n",
        "#Getting the keys and secret from developer account\n",
        "consumer_key = \"CS9tZKW7OigvRwDgVO3jk6EuI\"\n",
        "consumer_secret = \"MGZwRe1s5NqXRNQmL0inQpUc1mcVGfHumdwqBpnAY9gK42cFA5\"\n",
        "access_token = \"870844765805346820-F5Na46w4OSAWyk9cQHCppWifDnNbIdG\"\n",
        "access_token_secret = \"VVloBkwHuSLuioXkDQwRMeF15tQrRgDYI0aH6ZzjdA1rD\"\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "\n",
        "tweets = tweepy.Cursor(api.search,q=text_query,lang='en').items(max_tweets)\n",
        " \n",
        "# This will create a nested list of all the attributes in the tweets\n",
        "tweets_list = [[tweet.text, tweet.created_at, tweet.id_str, tweet.user.name, tweet.user.screen_name, \n",
        "                tweet.user.id_str, tweet.user.location, tweet.user.url, tweet.user.description,\n",
        "                tweet.user.verified,tweet.user.followers_count, tweet.user.friends_count, \n",
        "                tweet.user.favourites_count,tweet.user.statuses_count, tweet.user.listed_count,\n",
        "                tweet.user.created_at, tweet.user.profile_image_url_https, \n",
        "                tweet.user.default_profile,tweet.user.default_profile_image] for tweet in tweets]\n",
        " \n",
        "tweets_df = pd.DataFrame(tweets_list) #creating the dataframe \n",
        "tweets_df\n",
        "\n",
        "#printing the result as per the question\n",
        "Tweets = pd.DataFrame()\n",
        "Tweets['User_Name'] = tweets_df[4]\n",
        "Tweets['Posted_Time'] = tweets_df[1]\n",
        "Tweets['Text'] = tweets_df[0]\n",
        "Tweets\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_Name</th>\n",
              "      <th>Posted_Time</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tesla_Bill</td>\n",
              "      <td>2021-10-13 18:47:56</td>\n",
              "      <td>RT @au_tom_otive: I have no idea why any OEM d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BurksArnetta</td>\n",
              "      <td>2021-10-13 18:47:52</td>\n",
              "      <td>Our FREE 2021 TESLA Giveaway is LIVE.\\n #doge ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WholeMarsBlog</td>\n",
              "      <td>2021-10-13 18:47:52</td>\n",
              "      <td>RT @paorfull: @WholeMarsBlog Wrong, all of OEM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SalenaKerr1</td>\n",
              "      <td>2021-10-13 18:47:51</td>\n",
              "      <td>Our FREE 2021 TESLA Giveaway is LIVE.\\n #doge ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>_GirlMaher_</td>\n",
              "      <td>2021-10-13 18:47:50</td>\n",
              "      <td>Now prove to all the puppets that you know whe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>TNR_Gold</td>\n",
              "      <td>2021-10-13 18:25:01</td>\n",
              "      <td>RT @kirillklip: Now Rob #McEwen is doing exact...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>icungipark</td>\n",
              "      <td>2021-10-13 18:25:00</td>\n",
              "      <td>RT @CryptoKing1st: EARLY ACCESS SALES ENDS TON...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>humoud_khaled</td>\n",
              "      <td>2021-10-13 18:24:56</td>\n",
              "      <td>RT @PsyJayCrypto: EARLY ACCESS SALES ENDS TONI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>BabyYohann</td>\n",
              "      <td>2021-10-13 18:24:56</td>\n",
              "      <td>RT @CryptoKing1st: EARLY ACCESS SALES ENDS TON...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>pickavacityx_</td>\n",
              "      <td>2021-10-13 18:24:53</td>\n",
              "      <td>RT @CryptoKing1st: EARLY ACCESS SALES ENDS TON...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         User_Name  ...                                               Text\n",
              "0       Tesla_Bill  ...  RT @au_tom_otive: I have no idea why any OEM d...\n",
              "1     BurksArnetta  ...  Our FREE 2021 TESLA Giveaway is LIVE.\\n #doge ...\n",
              "2    WholeMarsBlog  ...  RT @paorfull: @WholeMarsBlog Wrong, all of OEM...\n",
              "3      SalenaKerr1  ...  Our FREE 2021 TESLA Giveaway is LIVE.\\n #doge ...\n",
              "4      _GirlMaher_  ...  Now prove to all the puppets that you know whe...\n",
              "..             ...  ...                                                ...\n",
              "995       TNR_Gold  ...  RT @kirillklip: Now Rob #McEwen is doing exact...\n",
              "996     icungipark  ...  RT @CryptoKing1st: EARLY ACCESS SALES ENDS TON...\n",
              "997  humoud_khaled  ...  RT @PsyJayCrypto: EARLY ACCESS SALES ENDS TONI...\n",
              "998     BabyYohann  ...  RT @CryptoKing1st: EARLY ACCESS SALES ENDS TON...\n",
              "999  pickavacityx_  ...  RT @CryptoKing1st: EARLY ACCESS SALES ENDS TON...\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}